# How to download files 

-Norah recommended downloading manually onto Lacie hard drive, worked but took a long time!
-Now that all are on Lacie, tried using Cyber Duck to get them onto CHPC - did not work. do i need to be using the U's VPN?
-Tried Globus but that didn't seem to work either 
-Now trying new method going directly form web server to CHPC- this is what I was thinking originally and it worked well, a little slow

wget -r -np -nd -A "fastq,fastq.gz" \
? http://fcb.ycga.yale.edu:3010/TKlineFkigyao4idvY09P1b0xrr__cg/sample_dir_000019646/
--2025-11-21 12:31:32--  http://fcb.ycga.yale.edu:3010/TKlineFkigyao4idvY09P1b0xrr__cg/sample_dir_000019646/

12/3/25
forgot to write about running fastqc and multiqc but it was easy added to 
/uufs/chpc.utah.edu/common/home/u1055819/saarman-group/Cx_WGS/fastQC_WGS_Cx


12/25 Conversations with Andrea and Zach about phasing and reference genomes etc. decided to use same USA Cx pipiens genome which we used for ddRAD project which exists here 
my $genome = "/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/ref/Cpip29_nodeb_gfill_mito.fasta";

It is currently unpublished. Hopefully by the time I am ready to write they will be further along? 

1/20/26
Been working on getting bwa-mem running for these samples. Took lots of troubleshooting. 

1/25/26 
Think I finally made progress - running test files best run so far is 4 hours in right now here is slurm output file 	slurm-10681360.out, output bams will go in Cx_WGS_bwa

2/10/26 
Successfully ran steps 3! Made a summary file of all the flagstats in python. Duplicates are marked and look within normal range. Don't actually have to remove them can just use:
bcftools mpileup with the --skip-duplicates flag (or -d for short)
This tells bcftools to ignore reads with the duplicate flag during variant calling


2/11/26
Realized it probably makes sense to merge files of same sample from different lanes .... changed step 3 to a merge scripts then will rerun samtools step for markdups
checking bam list 

All BAM files: 99 (with duplicate lanes)
Merged BAMs: 65 (unique biological samples)
Difference: 99 - 65 = 34 extra files
This means 34 samples had multiple lanes that got merged together.

Let me count the duplicates from your list to verify:
Samples with 2 lanes (pairs):

BE_202_179_014 (L001 + L004)
BE_203_167_026 (L001 + L004)
BE_204_155_038 (L004 + L001)
Cache_121_131_062 (L001 + L004)
Cache_122_119_074 (L001 + L004)
Cache_123_107_086 (L001 + L004)
OGD_802_178_015 (L004 + L001)
OGD_805_166_027 (L004 + L001)
OGD_806_154_039 (L004 + L001)
OGD_807_142_051 (L004 + L001)
OGD_809_130_063 (L004 + L001)
SGE_010_118_075 (L004 + L001)
SGE_015_177_016 (L004 + L001)
SLC_053_141_052 (L004 + L001)
SLC_074_129_064 (L004 + L001)
SLC_263_105_088 (L004 + L001)
SLC_372_188_005 (L004 + L001)
SLC_379_128_065 (L004 + L001)
SLC_380_116_077 (L004 + L001)
SLC_523_104_089 (L004 + L001)
UTCO_103_175_018 (L004 + L001)
UTCO_104_163_030 (L004 + L001)
UTCO_112_139_054 (L004 + L001)
UTCO_117_127_066 (L004 + L001)
UTCO_119_103_090 (L004 + L001)
UTCO_122_174_019 (L004 + L001)
UTCO_123_161_032 (L008 + L001)
UTCO_124_149_044 (L008 + L001)
UTCO_125_138_055 (L004 + L001)
UTCO_132_137_056 (L008 + L001)
UTCO_134_113_080 (L008 + L001)
UTCO_136_185_008 (L004 + L001)
WMAD_005_125_068 (L008 + L001)
WMAD_011_161_032 (L004 + L001)

2/12/26
Updated array and running step 4 samtools markdups again


2/13/26
running tables to investigate read depth and pcr dups to determine what to use for bcf filters 


